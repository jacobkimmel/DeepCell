{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Train CNNs for use with @jkimmel's fork of @vanvalen's DeepCell\n",
    "\n",
    "DeepCell trains standard CNNs on small receptive fields, then\n",
    "transfers the weights from these standard networks to a corresponding\n",
    "architecture that uses 'dilated' or 'atrous' kernels which operate\n",
    "on full sized images. \n",
    "\n",
    "Use of 'atrous' kernels on a large image is equivalent to processing\n",
    "each receptive field 'patchwise', such that weights can be transferred\n",
    "from these small, quick-to-train networks, over to the atrous kernel\n",
    "network. \n",
    "The foundations behind this approach are described here:\n",
    "https://arxiv.org/abs/1412.4526\n",
    "\n",
    "This notebook outlines how to train a vanilla CNN on prepared \n",
    "training data, as described in 00_generate_training_data.ipynb\n",
    "'''\n",
    "\n",
    "from __future__ import print_function, division #python2 compatability\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from cnn_functions import rate_scheduler, train_model_sample\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Import the model you wish to train\n",
    "# DeepCell models are listed in the model zoo, each with a specific\n",
    "# receptive field size\n",
    "# As a starting point, try the batch normalized model for your desired\n",
    "# receptive field size\n",
    "# i.e. bn_feature_net_NNxNN\n",
    "from model_zoo import bn_feature_net_81x81 as the_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a batch size and the number of epochs to run\n",
    "# batch sizes should be set as large as possible, with GPU RAM being\n",
    "# the limiting factor\n",
    "# larger batchsizes improves gradient estimation, usually improving training\n",
    "batch_size = 256\n",
    "# number of times the network will be shown the same training data\n",
    "n_epoch = 50\n",
    "\n",
    "# Specify the name of the dataset \n",
    "# i.e. the filename of the training data, without the extension\n",
    "dataset = 'training_data'\n",
    "# set a name for the experiment\n",
    "expt = 'exp_name'\n",
    "\n",
    "# set the directory to save the model weights\n",
    "direc_save = '/path/to/saved/models/'\n",
    "# directory containing the dataset\n",
    "direc_data = '/path/to/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the optimizer\n",
    "# SGD works best for batchnorm nets, while RMSprop seems to be better\n",
    "# for non-normalized nets\n",
    "# here we set SGD with nesterov momentum and a scheduled decay\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr = 0.01, decay = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set how many models you'd like to train\n",
    "# training multiple models may improve prediction performance,\n",
    "# based on the ensemble effect\n",
    "# see:\n",
    "# https://en.wikipedia.org/wiki/Boosting_(machine_learning)?oldformat=true\n",
    "nb_models = 1\n",
    "\n",
    "for iterate in range(1,nb_models):\n",
    "\n",
    "    model = the_model(n_channels = 1, n_features = 2, reg = 1e-5)\n",
    "\n",
    "    train_model_sample(model = model, dataset = dataset, optimizer = optimizer,\n",
    "        expt = expt, it = iterate, batch_size = batch_size, n_epoch = n_epoch,\n",
    "        direc_save = direc_save,\n",
    "        direc_data = direc_data,\n",
    "        lr_sched = lr_sched,\n",
    "        rotate = True, flip = True, shear = False)\n",
    "\n",
    "    del model\n",
    "    # reset the keras numbering scheme to ensure layers are named properly\n",
    "    # when training >1 model in a run\n",
    "    from keras.backend.common import _UID_PREFIXES\n",
    "    for key in _UID_PREFIXES.keys():\n",
    "        _UID_PREFIXES[key] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
